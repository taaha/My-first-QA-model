{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUGE\n",
    "\n",
    "**ROUGE** stands for **R**ecall-**O**riented **U**nderstudy for **G**isting **E**valuation. We now implement this metric in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'r': 1.0, 'p': 0.5, 'f': 0.6666666622222223},\n",
       "  'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
       "  'rouge-l': {'r': 1.0, 'p': 0.5, 'f': 0.6666666622222223}}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "model_out = 'hello to the world'\n",
    "reference = 'hello world'\n",
    "\n",
    "# initialize the rouge object\n",
    "rouge = Rouge()\n",
    "\n",
    "# get the scores\n",
    "rouge.get_scores(model_out, reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_scores` method returns three metrics, ROUGE-N using a unigram (ROUGE-1) and a bigram (ROUGE-2) â€” and ROUGE-L.\n",
    "\n",
    "For each of these, we receive the F1 score **f**, precision **p**, and recall **r**.\n",
    "\n",
    "Let's apply this to our set of five answers and see what we get. First, we need to define the `answers` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = [{'predicted': 'France', 'true': 'France.'},\n",
    "           {'predicted': 'in the 10th and 11th centuries',\n",
    "            'true': '10th and 11th centuries'},\n",
    "           {'predicted': '10th and 11th centuries', 'true': '10th and 11th centuries'},\n",
    "           {'predicted': 'Denmark, Iceland and Norway',\n",
    "            'true': 'Denmark, Iceland and Norway'},\n",
    "           {'predicted': 'Rollo', 'true': 'Rollo,'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to reformat this list into two lists, one for our predictions `model_out` and another for the true answers `reference`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out = [ans['predicted'] for ans in answers]\n",
    "\n",
    "reference = [ans['true'] for ans in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['France',\n",
       " 'in the 10th and 11th centuries',\n",
       " '10th and 11th centuries',\n",
       " 'Denmark, Iceland and Norway',\n",
       " 'Rollo']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can pass both of these lists to the `rouge.get_scores` method to return a list of results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995},\n",
       "  'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
       "  'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}},\n",
       " {'rouge-1': {'r': 1.0, 'p': 0.6666666666666666, 'f': 0.7999999952000001},\n",
       "  'rouge-2': {'r': 1.0, 'p': 0.6, 'f': 0.7499999953125},\n",
       "  'rouge-l': {'r': 1.0, 'p': 0.6666666666666666, 'f': 0.7999999952000001}},\n",
       " {'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995},\n",
       "  'rouge-2': {'r': 1.0, 'p': 1.0, 'f': 0.999999995},\n",
       "  'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}},\n",
       " {'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995},\n",
       "  'rouge-2': {'r': 1.0, 'p': 1.0, 'f': 0.999999995},\n",
       "  'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}},\n",
       " {'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
       "  'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
       "  'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.get_scores(model_out, reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we want to get average metrics for all answers, we can do this by adding `avg=True` to the `get_scores` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.8, 'p': 0.7333333333333333, 'f': 0.7599999960400001},\n",
       " 'rouge-2': {'r': 0.6, 'p': 0.52, 'f': 0.5499999970625},\n",
       " 'rouge-l': {'r': 0.8, 'p': 0.7333333333333333, 'f': 0.7599999960400001}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.get_scores(model_out, reference, avg=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
